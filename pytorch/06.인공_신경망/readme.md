# 데이터
- 전체 데이터 = 훈련 + 검증 + 테스트
- 검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도임.
- 하이퍼파라미터(초매개변수)란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 말함. 반면, 가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 매개변수라고 부름.
- 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수. 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값.

# 분류와 회귀
- 이진 분류는 주어진 입력에 대해서 둘 중 하나의 답을 정하는 문제.
- 다중 클래스 분류는 주어진 입력에 대해서 세 개 이상의 정해진 선택지 중에서 답을 정하는 문제.
- 회귀 문제는 분류 문제처럼 0 또는 1이나 과학 책장, IT 책장 등과 같이 분리된(비연속적인) 답이 결과가 아니라 연속된 값을 결과로 가짐.

# 지도 학습과 비지도 학습
- 지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것을 말함.
- 비지도 학습은 기본적으로 목적 데이터(또는 레이블)이 없는 학습 방법.
- 강화 학습은 어떤 환경 내에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법.

# 혼동 행렬
- 머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 함. 하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않음. 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix).
- 성(Positive)과 음성(Negative)을 구분하는 이진 분류가 있다고 하였을 때 혼동 행렬은 다음과 같음.
  
![](../img/06.1readme.png)
- 정밀도은 양성이라고 대답한 전체 케이스에 대한 TP의 비율.

![](../img/06.2readme.png)

- 재현률은 실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율임. 즉, 양성인 데이터 중에서 얼마나 양성인지를 예측(재현)했는지를 나타냄.

![](../img/06.3readme.png)

# 과적합과 과소 적합
- 과적합(Overfitting)이란 훈련 데이터를 과하게 학습한 경우를 말함. 
- 예를 들어 강아지 사진과 고양이 사진을 구분하는 기계가 있을 때, 검은색 강아지 사진 훈련 데이터를 과하게 학습하면 기계는 나중에 가서는 흰색 강아지나, 갈색 강아지를 보고도 강아지가 아니라고 판단하게 되는데, 이는 훈련 데이터에 대해서 지나친 일반화를 한 상황임.
- 과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생함.

![](../img/06.4readme.png)
- 과적합 방지를 위해 테스트 데이터의 성능이 낮아지기 전에 훈련을 멈추는 것이 바람직하다고 했는데, 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 반대로 과소적합(Underfitting)이라고함.
- 소 적합은 훈련 자체가 부족한 상태이므로 과대 적합과는 달리 훈련 데이터에 대해서도 보통 정확도가 낮다는 특징이 있음.

# 퍼셉트론
- x는 입력값을 의미하며, W는 가중치(Weight), y는 출력값입니다. 그림 안의 원은 인공 뉴런에 해당됨. 실제 신경 세포 뉴런에서의 신호를 전달하는 축삭돌기의 역할을 퍼셉트론에서는 가중치가 대신함. 각각의 인공 뉴런에서 보내진 입력값 x는 각각의 가중치 W와 함께 종착지인 인공 뉴런에 전달되고 있음. 편향 b도 포함됨.

![](../img/06.5readme.png)
- 각 입력값이 가중치와 곱해져서 인공 뉴런에 보내지고, 각 입력값과 그에 해당되는 가중치의 곱의 전체 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로서 1을 출력하고, 그렇지 않을 경우에는 0을 출력. 러한 함수를 계단 함수(Step function)라고 함.

![](../img/06.6readme.png)

# 다중 퍼셉트론
- 다층 퍼셉트론과 단층 퍼셉트론의 차이는 단층 퍼셉트론은 입력층과 출력층만 존재하지만, 다층 퍼셉트론은 중간에 층을 더 추가하였다는 점임. 이렇게 입력층과 출력층 사이에 존재하는 층을 은닉층(hidden layer)이라고 함.  다층 퍼셉트론은 줄여서 MLP라고도 부름.
- 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 함. 심층 신경망은 다층 퍼셉트론만 이야기 하는 것이 아니라, 여러 변형된 다양한 신경망들도 은닉층이 2개 이상이 되면 심층 신경망이라고 함.

# 순전파
- 각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됨. 이를 출력층 뉴런까지 진행.
- 오차(Error)를 계산하기 위한 손실 함수(Loss function)로는 평균 제곱 오차 MSE를 사용함.

![](../img/06.7readme.png)

# 역전파
- 순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해감. 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 업데이트하는 단계를 역전파 1단계, 그리고 N층과 N층의 이전층 사이의 가중치를 업데이트 하는 단계를 역전파 2단계라고 해봄.

![](../img/06.8readme.png)