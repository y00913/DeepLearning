# 원-핫 인코딩
- 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식임.
- ex) [0, 0, 1, 0, 0, 0]  

# 원-핫 인코딩의 한계
- 이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있음.
- 단어의 유사도를 표현하지 못한다는 단점이 있음.

# 희소 표현
- 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 함. 원-핫 벡터는 희소 벡터(sparse vector)임.
- 이러한 벡터 표현은 공간적 낭비를 불러일으킴. 원-핫 벡터의 문제점은 단어 벡터 간 유사도를 표현할 수 없다는 점도 있슴.

# 밀집 표현
- 이러한 희소 표현과 반대되는 표현이 있으니, 이를 밀집 표현(dense representation)이라고 함.
- 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않고 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤.
- ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...]
  
# 워드 임베딩
- 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 함. 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 함.
- 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있음.

# 워드투벡터
- https://wikidocs.net/60854

# 글로브
- 글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법론.
- GloVe는 이러한 기존 방법론들의 각각의 한계를 지적하며, LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 두 가지를 모두 사용함.

# 윈도우 기반 동시 등장 행렬
- 단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬을 말함.
- 윈도우 크기가 N일 때는 좌, 우에 존재하는 N개의 단어만 참고하게 됨.

# 동시 등장 확률
- 동시 등장 확률 P(K|i)는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률임.
- P(K|i)에서 i를 중심 단어(Center Word), k를 주변 단어(Context Word)라고 했을 때, 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값이라고 볼 수 있음.

