# 경사 하강법

- 경사 하강법은 비용 함수를 미분하여 이 함수의 기울기(gradient)를 구해서 비용이 최소화 되는 방향을 찾아내는 알고리즘.
- 비용 함수 = 손실 함수 = 오차 함수
- 파이토치에서 자동 미분을 지원하여 경사 하강법을 쉽게 사용할 수 있음.

# forword

- H(x) 식에 입력 x로부터 예측된 y를 얻는 것을 forward 연산이라고 함.
- 학습 전, prediction = model(x_train)은 x_train으로부터 예측값을 리턴하므로 forward 연산.
- 학습 후, pred_y = model(new_var)는 임의의 값 new_var로부터 예측값을 리턴하므로 forward 연산.

# backward

- 학습 과정에서 비용 함수를 미분하여 기울기를 구하는 것을 backward 연산이라고 함.
- cost.backward()는 비용 함수로부터 기울기를 구하라는 의미이며 backward 연산임.

# 미니 배치
- 전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법을 '배치 경사 하강법'이라고 함. 반면, 미니 배치 단위로 경사 하강법을 수행하는 방법을 '미니 배치 경사 하강법'이라고 함.
- 배치 경사 하강법은 경사 하강법을 할 때, 전체 데이터를 사용하므로 가중치 값이 최적값에 수렴하는 과정이 매우 안정적이지만, 계산량이 너무 많이 듬. 미니 배치 경사 하강법은 경사 하강법을 할 때, 전체 데이터의 일부만을 보고 수행하므로 최적값으로 수렴하는 과정에서 값이 조금 헤매기도 하지만 훈련 속도가 빠름.
- 배치 크기는 보통 2의 제곱수를 사용. ex) 2, 4, 8, 16, 32, 64... 그 이유는 CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 경우에 데이터 송수신의 효율을 높일 수 있다고 함.
- 이터레이션은 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 W와 b의 업데이트 횟수임. 전체 데이터가 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10개. 이는 한 번의 에포크 당 매개변수 업데이트가 10번 이루어짐을 의미함.
